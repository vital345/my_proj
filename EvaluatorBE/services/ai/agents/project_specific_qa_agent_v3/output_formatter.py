
from typing import List,Literal,Optional
from langgraph.prebuilt import create_react_agent
from langchain.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel
from langchain.prompts import ChatPromptTemplate
from pydantic import Field

class Question(BaseModel):
    # """Question to be asked to the user"""
    question:str = Field(description="question text you want to ask the user")
    code_snippet:Optional[str] = Field(description="An optimal code snippet related to the question.")
    
class FinalReport(BaseModel):
    """Final report after the evaluation process"""
    score:int = Field(description="final score out of 10 at the end of evaluation process")
    explanation:str = Field(description="brief explanation  explaining the score and highlight the user's strengths and areas for improvement.")
    
class Output(BaseModel):
    output: FinalReport | Question

 




def output_formatter(raw_report:str):
    llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0,
   )
    
    examples = """
{
    "output": {
        "question":"what does the following code snippet do",
        "code_snippet": "def main(): ....."
    }
}
----------
{
    "output":{
        "score":6,
        "explanation": "the user answered all the questions correctly"
    }
}
"""
    
    system_prompt = f"""
You are a formatting agent responsible for taking the raw output generated by a previous questioning agent as an input. 
Your task is to reformat the output of the previous agent to the specified format.

Example Outputs:
{examples}

Output of the previous Agent:
{raw_report}
""" 

    llm = llm.with_structured_output(Output)
    # prompt = ChatPromptTemplate.from_messages([
    #        ("system",system_prompt)
    # ])
    return llm.invoke([("system",system_prompt)])    
        
   
   
    