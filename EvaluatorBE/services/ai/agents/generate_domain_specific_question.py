from typing import List
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field
from langchain.prompts import ChatPromptTemplate


system_prompt = """
You are an intelligent agent who generate list questions based on the requirement by the user.

Number of questions to be generated: {number_of_questions}

Requirement:
{requirement}

"""


class OutputFormat(BaseModel):
    questions: List[str] = Field(description="List of domain-specific questions generated by the AI. It should be list of just question string.")


async def question_generator(requirement: str, number_of_questions: int) -> OutputFormat:
    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0.8,
    )

    llm = llm.with_structured_output(OutputFormat)
    prompt = ChatPromptTemplate.from_messages([("system", system_prompt)])

    chain = prompt | llm
    for i in range(3):
        try:
            questions: OutputFormat = await chain.ainvoke({"requirement": requirement, "number_of_questions": number_of_questions})
            print(questions)
            return questions.dict()
        except Exception as e:
            print("Error in question generator")
            print(e)
            continue


if __name__ == "__main__":
    project_requirement = """
    Python
    """
    import asyncio
    asyncio.run(question_generator(project_requirement, 20))
